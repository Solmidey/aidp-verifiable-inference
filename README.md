## Overview
Decentralized, deterministic AI inference running on AIDP GPUs.

## Why GPU Compute?
LLM inference requires GPU acceleration for latency and cost efficiency.

## Verifiable Inference
Each inference returns a cryptographic receipt allowing reproducibility.

## AIDP Integration
All inference workloads are executed on AIDP decentralized GPU nodes.

## Future Work
- ZK proof of inference correctness
- Onchain verification
- Agent consumption layer
